/* Generated by lib/Fixed256-aarch64.py */

/* Copyright Â© Richard Kettlewell.
*
* This program is free software: you can redistribute it and/or modify
* it under the terms of the GNU General Public License as published by
* the Free Software Foundation, either version 3 of the License, or
* (at your option) any later version.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
* GNU General Public License for more details.
*
* You should have received a copy of the GNU General Public License
* along with this program.  If not, see <http://www.gnu.org/licenses/>.
*/
#include <config.h>

#if __aarch64__
#define SYMBOL(s) s

.text
.align 4
.globl SYMBOL(Fixed256_mul)
SYMBOL(Fixed256_mul):
// Load operands
  // x6:x5:x4:x3 <- [x1]
  ldp x3,x4,[x1]
  ldp x5,x6,[x1,16]
  // x14:x13:x12:x11 <- [x2]
  ldp x11,x12,[x2]
  ldp x13,x14,[x2,16]
// Absolute value of a
  tbz x6,#63,1f
  // negate x6:x5:x4:x3
  negs x3,x3
  ngcs x4,x4
  ngcs x5,x5
  ngc x6,x6
  eor x0,x0,#1
1:
// Absolute value of b
  tbz x14,#63,2f
  // negate x14:x13:x12:x11
  negs x11,x11
  ngcs x12,x12
  ngcs x13,x13
  ngc x14,x14
  eor x0,x0,#1
2:
// Compute r[-3] using x10:x9:x8
  // x9:<ignored> <- a[0] * b[0]
  umulh x9,x3,x11
// Compute r[-2] using x7:x10:x9
  // x2:x1 <- a[0] * b[1]
  mul x1,x3,x12
  umulh x2,x3,x12
  // x7:x10:x9 += a[0] * b[1]
  adds x9,x9,x1
  adcs x10,xzr,x2
  adc x7,xzr,xzr
  // x2:x1 <- a[1] * b[0]
  mul x1,x4,x11
  umulh x2,x4,x11
  // x7:x10:x9 += a[1] * b[0]
  adds x9,x9,x1
  adcs x10,x10,x2
  adc x7,x7,xzr
// Compute r[-1] using x8:x7:x10
  // x2:x1 <- a[0] * b[2]
  mul x1,x3,x13
  umulh x2,x3,x13
  // x8:x7:x10 += a[0] * b[2]
  adds x10,x10,x1
  adcs x7,x7,x2
  adc x8,xzr,xzr
  // x2:x1 <- a[1] * b[1]
  mul x1,x4,x12
  umulh x2,x4,x12
  // x8:x7:x10 += a[1] * b[1]
  adds x10,x10,x1
  adcs x7,x7,x2
  adc x8,x8,xzr
  // x2:x1 <- a[2] * b[0]
  mul x1,x5,x11
  umulh x2,x5,x11
  // x8:x7:x10 += a[2] * b[0]
  adds x10,x10,x1
  adcs x7,x7,x2
  adc x8,x8,xzr
// Round up
  adds xzr,x10,x10
  adcs x7,x7,xzr
  adc x8,x8,xzr
// Compute r[0] using x9:x8:x7
  // x2:x1 <- a[0] * b[3]
  mul x1,x3,x14
  umulh x2,x3,x14
  // x9:x8:x7 += a[0] * b[3]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,xzr,xzr
  // x2:x1 <- a[1] * b[2]
  mul x1,x4,x13
  umulh x2,x4,x13
  // x9:x8:x7 += a[1] * b[2]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,x9,xzr
  // x2:x1 <- a[2] * b[1]
  mul x1,x5,x12
  umulh x2,x5,x12
  // x9:x8:x7 += a[2] * b[1]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,x9,xzr
  // x2:x1 <- a[3] * b[0]
  mul x1,x6,x11
  umulh x2,x6,x11
  // x9:x8:x7 += a[3] * b[0]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,x9,xzr
// Compute r[1] using x10:x9:x8
  // x2:x1 <- a[1] * b[3]
  mul x1,x4,x14
  umulh x2,x4,x14
  // x10:x9:x8 += a[1] * b[3]
  adds x8,x8,x1
  adcs x9,x9,x2
  adc x10,xzr,xzr
  // x2:x1 <- a[2] * b[2]
  mul x1,x5,x13
  umulh x2,x5,x13
  // x10:x9:x8 += a[2] * b[2]
  adds x8,x8,x1
  adcs x9,x9,x2
  adc x10,x10,xzr
  // x2:x1 <- a[3] * b[1]
  mul x1,x6,x12
  umulh x2,x6,x12
  // x10:x9:x8 += a[3] * b[1]
  adds x8,x8,x1
  adcs x9,x9,x2
  adc x10,x10,xzr
// Compute r[2] using x10:x9
  // x2:x1 <- a[2] * b[3]
  mul x1,x5,x14
  umulh x2,x5,x14
  // x10:x9 += a[2] * b[3]
  adds x9,x9,x1
  adc x10,x10,x2
  // x2:x1 <- a[3] * b[2]
  mul x1,x6,x13
  umulh x2,x6,x13
  // x10:x9 += a[3] * b[2]
  adds x9,x9,x1
  adc x10,x10,x2
// Compute r[3] using x10
  // x10 += a[3] * b[3]
  madd x10,x6,x14,x10
// Set sign
  tbz x0,#0,3f
  // negate x10:x9:x8:x7
  negs x7,x7
  ngcs x8,x8
  ngcs x9,x9
  ngc x10,x10
  eor x0,x0,#1
3:
// Store result
  // [x0] <- x10:x9:x8:x7
  stp x7,x8,[x0]
  stp x9,x10,[x0,16]
  ret
.text
.align 4
.globl SYMBOL(Fixed256_square)
SYMBOL(Fixed256_square):
// Load operands
  // x6:x5:x4:x3 <- [x1]
  ldp x3,x4,[x1]
  ldp x5,x6,[x1,16]
// Absolute value of a
  tbz x6,#63,1f
  // negate x6:x5:x4:x3
  negs x3,x3
  ngcs x4,x4
  ngcs x5,x5
  ngc x6,x6
1:
// Compute r[-3] using x10:x9:x8
  // x9:<ignored> <- a[0] * b[0]
  umulh x9,x3,x3
// Compute r[-2] using x7:x10:x9
  // x2:x1 <- a[1] * b[0]
  mul x1,x4,x3
  umulh x2,x4,x3
  // x7:x10:x9 += a[1] * b[0]
  adds x9,x9,x1
  adcs x10,xzr,x2
  adc x7,xzr,xzr
  // x7:x10:x9 += a[0] * b[1]
  adds x9,x9,x1
  adcs x10,x10,x2
  adc x7,x7,xzr
// Compute r[-1] using x8:x7:x10
  // x2:x1 <- a[1] * b[1]
  mul x1,x4,x4
  umulh x2,x4,x4
  // x8:x7:x10 += a[1] * b[1]
  adds x10,x10,x1
  adcs x7,x7,x2
  adc x8,xzr,xzr
  // x2:x1 <- a[2] * b[0]
  mul x1,x5,x3
  umulh x2,x5,x3
  // x8:x7:x10 += a[2] * b[0]
  adds x10,x10,x1
  adcs x7,x7,x2
  adc x8,x8,xzr
  // x8:x7:x10 += a[0] * b[2]
  adds x10,x10,x1
  adcs x7,x7,x2
  adc x8,x8,xzr
// Round up
  adds xzr,x10,x10
  adcs x7,x7,xzr
  adc x8,x8,xzr
// Compute r[0] using x9:x8:x7
  // x2:x1 <- a[2] * b[1]
  mul x1,x5,x4
  umulh x2,x5,x4
  // x9:x8:x7 += a[2] * b[1]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,xzr,xzr
  // x9:x8:x7 += a[1] * b[2]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,x9,xzr
  // x2:x1 <- a[3] * b[0]
  mul x1,x6,x3
  umulh x2,x6,x3
  // x9:x8:x7 += a[3] * b[0]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,x9,xzr
  // x9:x8:x7 += a[0] * b[3]
  adds x7,x7,x1
  adcs x8,x8,x2
  adc x9,x9,xzr
// Compute r[1] using x10:x9:x8
  // x2:x1 <- a[2] * b[2]
  mul x1,x5,x5
  umulh x2,x5,x5
  // x10:x9:x8 += a[2] * b[2]
  adds x8,x8,x1
  adcs x9,x9,x2
  adc x10,xzr,xzr
  // x2:x1 <- a[3] * b[1]
  mul x1,x6,x4
  umulh x2,x6,x4
  // x10:x9:x8 += a[3] * b[1]
  adds x8,x8,x1
  adcs x9,x9,x2
  adc x10,x10,xzr
  // x10:x9:x8 += a[1] * b[3]
  adds x8,x8,x1
  adcs x9,x9,x2
  adc x10,x10,xzr
// Compute r[2] using x10:x9
  // x2:x1 <- a[3] * b[2]
  mul x1,x6,x5
  umulh x2,x6,x5
  // x10:x9 += a[3] * b[2]
  adds x9,x9,x1
  adc x10,x10,x2
  // x10:x9 += a[2] * b[3]
  adds x9,x9,x1
  adc x10,x10,x2
// Compute r[3] using x10
  // x10 += a[3] * b[3]
  madd x10,x6,x6,x10
// Store result
  // [x0] <- x10:x9:x8:x7
  stp x7,x8,[x0]
  stp x9,x10,[x0,16]
  ret
#endif

